# -*- coding: utf-8 -*-
"""prediksi komentar 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ML0taTqRJK32ozb5RD9On_vLc1RSllr-
"""

import pandas as pd
import re
import string
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

# Download resource NLTK jika belum ada
nltk.download('stopwords')
nltk.download('punkt')

# --- 1. Setup Stopwords ---
# Ambil stopwords bawaan bahasa Indonesia
stop_words = set(stopwords.words('indonesian'))

# PENTING: Jangan hapus kata-kata negasi & waktu!
# Tanpa stemming, kita harus lebih hati-hati membuang kata.
keep_words = {
    "tidak", "tapi", "namun", "bukan", "jangan", "belum", "kurang",
    "telah", "sudah", "bisa", "dapat", "akan", "sedang"
}
stop_words = stop_words - keep_words

# Tambahkan kata sampah (interjeksi) yang aman dihapus
extra_stopwords = {
    "nih", "sih", "dong", "nya", "tuh", "deh", "kok", "aja", "saja",
    "yah", "wow", "oh", "hm", "hmm", "yup", "gan", "kak", "bro", "sis",
    "yuk", "loh", "kan", "yang", "dan", "di", "ke", "dari"
}
stop_words.update(extra_stopwords)

# --- 2. Kamus Normalisasi (Pengganti Stemming) ---
# Kita gunakan kamus ini untuk membakukan kata tanpa memotong imbuhan secara paksa
normalisasi_dict = {
    # Negasi & Kata Sambung
    "gk": "tidak", "ga": "tidak", "gak": "tidak", "nggak": "tidak", "tdk": "tidak", "tak": "tidak",
    "klo": "kalau", "kalo": "kalau", "kl": "kalau",
    "dg": "dengan", "dgn": "dengan",
    "yg": "yang",
    "krn": "karena", "karna": "karena",
    "tp": "tapi",
    "sm": "sama",
    "utk": "untuk",
    "dlm": "dalam",
    "dr": "dari",

    # Pronoun (Kata Ganti)
    "sy": "saya", "gw": "saya", "aku": "saya", "gue": "saya", "gua": "saya",
    "lu": "kamu", "lo": "kamu", "u": "kamu",

    # Kata Kerja/Keterangan
    "jd": "jadi", "jdi": "jadi",
    "sdh": "sudah", "udh": "sudah", "dah": "sudah",
    "blm": "belum",
    "tlg": "tolong", "pls": "tolong", "mohon": "tolong", "bntu": "bantu",
    "knp": "kenapa", "napa": "kenapa",
    "gmn": "bagaimana", "gmna": "bagaimana",
    "bgt": "banget", "bngit": "banget", "bngt": "banget",
    "bkn": "bukan",
    "org": "orang",
    "dpt": "dapat",
    "lbh": "lebih",

    # Konteks Bencana (PENTING)
    "bnjr": "banjir",
    "evaku": "evakuasi",
    "korbn": "korban",
    "tnda": "tenda",
    "bntuan": "bantuan"
}

# Download resource NLTK jika belum ada
nltk.download('stopwords')
nltk.download('punkt')

# --- 1. Setup Stopwords ---
# Ambil stopwords bawaan bahasa Indonesia
stop_words = set(stopwords.words('indonesian'))

# PENTING: Jangan hapus kata-kata negasi & waktu!
# Tanpa stemming, kita harus lebih hati-hati membuang kata.
keep_words = {
    "tidak", "tapi", "namun", "bukan", "jangan", "belum", "kurang",
    "telah", "sudah", "bisa", "dapat", "akan", "sedang"
}
stop_words = stop_words - keep_words

# Tambahkan kata sampah (interjeksi) yang aman dihapus
extra_stopwords = {
    "nih", "sih", "dong", "nya", "tuh", "deh", "kok", "aja", "saja",
    "yah", "wow", "oh", "hm", "hmm", "yup", "gan", "kak", "bro", "sis",
    "yuk", "loh", "kan", "yang", "dan", "di", "ke", "dari"
}
stop_words.update(extra_stopwords)

# --- 2. Kamus Normalisasi (Pengganti Stemming) ---
# Kita gunakan kamus ini untuk membakukan kata tanpa memotong imbuhan secara paksa
normalisasi_dict = {
    # Negasi & Kata Sambung
    "gk": "tidak", "ga": "tidak", "gak": "tidak", "nggak": "tidak", "tdk": "tidak", "tak": "tidak",
    "klo": "kalau", "kalo": "kalau", "kl": "kalau",
    "dg": "dengan", "dgn": "dengan",
    "yg": "yang",
    "krn": "karena", "karna": "karena",
    "tp": "tapi",
    "sm": "sama",
    "utk": "untuk",
    "dlm": "dalam",
    "dr": "dari",

    # Pronoun (Kata Ganti)
    "sy": "saya", "gw": "saya", "aku": "saya", "gue": "saya", "gua": "saya",
    "lu": "kamu", "lo": "kamu", "u": "kamu",

    # Kata Kerja/Keterangan
    "jd": "jadi", "jdi": "jadi",
    "sdh": "sudah", "udh": "sudah", "dah": "sudah",
    "blm": "belum",
    "tlg": "tolong", "pls": "tolong", "mohon": "tolong", "bntu": "bantu",
    "knp": "kenapa", "napa": "kenapa",
    "gmn": "bagaimana", "gmna": "bagaimana",
    "bgt": "banget", "bngit": "banget", "bngt": "banget",
    "bkn": "bukan",
    "org": "orang",
    "dpt": "dapat",
    "lbh": "lebih",

    # Konteks Bencana (PENTING)
    "bnjr": "banjir",
    "evaku": "evakuasi",
    "korbn": "korban",
    "tnda": "tenda",
    "bntuan": "bantuan"
}

def train_and_save_model(file_path):
    """Melatih model klasifikasi dari data training."""

    # Memuat data (menggunakan delimiter ';' sesuai struktur file)
    try:
        df = pd.read_csv(file_path, delimiter=';')
    except Exception as e:
        print(f"Error saat memuat file: {e}")
        return None, None

    # Memastikan kolom yang dibutuhkan ada
    if 'full_text' not in df.columns or 'labels' not in df.columns:
        print("Pastikan file memiliki kolom 'full_text' dan 'labels'.")
        return None, None

    # Pembersihan teks
    df['clean_text'] = df['full_text'].apply(preprocess_text)

    # Mendefinisikan fitur (X) dan target (y)
    X = df['clean_text']
    y = df['labels']

    # Membuat Pipeline untuk Vektorizer dan Classifier
    # Model: Logistic Regression
    text_classifier = Pipeline([
        ('tfidf', TfidfVectorizer()),
        ('classifier', LogisticRegression(random_state=42, max_iter=1000))
    ])

    # Melatih model
    print("Mulai melatih model...")
    text_classifier.fit(X, y)
    print("Pelatihan model selesai.")

    # (Opsional) Evaluasi model di data training untuk melihat performa awal
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    text_classifier.fit(X_train, y_train)
    predictions = text_classifier.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    print("\n--- Akurasi: ", accuracy)
    print("---\n--- Laporan Klasifikasi (di Data Uji) ---")
    print(classification_report(y_test, predictions, target_names=['0 (Opini/Kritik)', '1 (Informasi)', '2 (Aksi/Bantuan)'], zero_division=0))

    return text_classifier

def classify_new_comment(model, new_texts):
    """Menggunakan model yang telah dilatih untuk memprediksi label komentar baru."""

    if model is None:
        print("Model tidak tersedia untuk prediksi.")
        return

    # Mapping Label
    label_map = {
        0: "0 (Opini/Keluhan/Kritik)",
        1: "1 (Informasi)",
        2: "2 (Aksi Pemerintah/Bantuan)"
    }

    # Memproses dan memprediksi
    print("\n--- Hasil Prediksi Komentar Terbaru ---")
    predictions = model.predict(new_texts)

    for text, label in zip(new_texts, predictions):
        print(f"\n[Komentar]: '{text}'")
        print(f"[Prediksi]: {label_map.get(label, 'Label Tidak Dikenal')}")

# --- Bagian Utama Program ---
if __name__ == "__main__":

    # 1. Path file data training Anda
    data_file = 'dataset_komentarbencana.csv'

    # 2. Melatih model
    classifier_model = train_and_save_model(data_file)

    # 3. Data Komentar Terbaru (yang ingin Anda klasifikasikan)
    new_comments = [
        "Banjir masih setinggi lutut",
        "Menurut saya hujan ini tidak akan berhenti sampai pagi",
        "Tim SAR berhasil mengevakuasi 50 warga yang terjebak banjir di perumahan Kemang Pratama tadi malam. #GerakCepat",
        "Bantuan yang diberikan tentara telah tiba",
        "pekerjaan dan gerak dari tim sar sangat lambat, saran saya perbaiki"
    ]

    # 4. Melakukan Klasifikasi
    if classifier_model:
        classify_new_comment(classifier_model, new_comments)