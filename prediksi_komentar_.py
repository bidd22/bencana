# -*- coding: utf-8 -*-
"""prediksi komentar 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kjeKmo1GQjtxRsG_V6qKUIZRYTTW6-RW
"""


import pandas as pd
import re
import string
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load Dataset
# Pastikan file berada di direktori yang sama atau sesuaikan path-nya
filename = 'dataset_komentarbencana.csv'
try:
    df = pd.read_csv(filename, sep=';') # Menggunakan delimiter titik koma sesuai dataset Anda
    print("Data berhasil dimuat!")
    print(df.info())
except FileNotFoundError:
    print(f"File {filename} tidak ditemukan.")

# Set style untuk plot
sns.set(style="whitegrid")

# ==========================================
# 2. Visualisasi Distribusi Label (Target)
# ==========================================
plt.figure(figsize=(8, 5))
ax = sns.countplot(x='labels', data=df, palette='viridis', hue='labels', legend=False)
plt.title('Distribusi Kelas (Labels)', fontsize=15)
plt.xlabel('Label (0=Opini, 1=Informasi, 2=Aksi)', fontsize=12)
plt.ylabel('Jumlah Data', fontsize=12)

# Menambahkan angka di atas bar
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')
plt.show()

# ==========================================
# 3. Visualisasi Distribusi Kota/Kabupaten
# ==========================================
plt.figure(figsize=(10, 6))
# Mengambil Top 10 kota terbanyak agar grafik rapi
top_cities = df['kota_kabupaten'].value_counts().nlargest(10).index
sns.countplot(y='kota_kabupaten', data=df[df['kota_kabupaten'].isin(top_cities)],
              order=top_cities, palette='magma', hue='kota_kabupaten', legend=False)
plt.title('Top 10 Asal Kota/Kabupaten', fontsize=15)
plt.xlabel('Jumlah', fontsize=12)
plt.ylabel('Kota/Kabupaten', fontsize=12)
plt.show()

# ==========================================
# 4. Analisis Karakteristik Teks
# ==========================================

# Hitung panjang karakter dan jumlah kata
df['text_length'] = df['full_text'].astype(str).apply(len)
df['word_count'] = df['full_text'].astype(str).apply(lambda x: len(x.split()))

# Plot Distribusi Panjang Teks (Karakter)
plt.figure(figsize=(10, 5))
sns.histplot(df['text_length'], bins=30, kde=True, color='skyblue')
plt.title('Distribusi Panjang Teks (Jumlah Karakter)', fontsize=15)
plt.xlabel('Panjang Teks', fontsize=12)
plt.ylabel('Frekuensi', fontsize=12)
plt.show()

# Plot Distribusi Jumlah Kata
plt.figure(figsize=(10, 5))
sns.histplot(df['word_count'], bins=30, kde=True, color='salmon')
plt.title('Distribusi Jumlah Kata per Komentar', fontsize=15)
plt.xlabel('Jumlah Kata', fontsize=12)
plt.ylabel('Frekuensi', fontsize=12)
plt.show()

# Download resource NLTK jika belum ada
nltk.download('stopwords')
nltk.download('punkt')

# --- 1. Setup Stopwords ---
# Ambil stopwords bawaan bahasa Indonesia
stop_words = set(stopwords.words('indonesian'))

# PENTING: Jangan hapus kata-kata negasi & waktu!
# Tanpa stemming, kita harus lebih hati-hati membuang kata.
keep_words = {
    "tidak", "tapi", "namun", "bukan", "jangan", "belum", "kurang",
    "telah", "sudah", "bisa", "dapat", "akan", "sedang"
}
stop_words = stop_words - keep_words

# Tambahkan kata sampah (interjeksi) yang aman dihapus
extra_stopwords = {
    "nih", "sih", "dong", "nya", "tuh", "deh", "kok", "aja", "saja",
    "yah", "wow", "oh", "hm", "hmm", "yup", "gan", "kak", "bro", "sis",
    "yuk", "loh", "kan", "yang", "dan", "di", "ke", "dari"
}
stop_words.update(extra_stopwords)

# --- 2. Kamus Normalisasi (Pengganti Stemming) ---
# Kita gunakan kamus ini untuk membakukan kata tanpa memotong imbuhan secara paksa
normalisasi_dict = {
    # Negasi & Kata Sambung
    "gk": "tidak", "ga": "tidak", "gak": "tidak", "nggak": "tidak", "tdk": "tidak", "tak": "tidak",
    "klo": "kalau", "kalo": "kalau", "kl": "kalau",
    "dg": "dengan", "dgn": "dengan",
    "yg": "yang",
    "krn": "karena", "karna": "karena",
    "tp": "tapi",
    "sm": "sama",
    "utk": "untuk",
    "dlm": "dalam",
    "dr": "dari",

    # Pronoun (Kata Ganti)
    "sy": "saya", "gw": "saya", "aku": "saya", "gue": "saya", "gua": "saya",
    "lu": "kamu", "lo": "kamu", "u": "kamu",

    # Kata Kerja/Keterangan
    "jd": "jadi", "jdi": "jadi",
    "sdh": "sudah", "udh": "sudah", "dah": "sudah",
    "blm": "belum",
    "tlg": "tolong", "pls": "tolong", "mohon": "tolong", "bntu": "bantu",
    "knp": "kenapa", "napa": "kenapa",
    "gmn": "bagaimana", "gmna": "bagaimana",
    "bgt": "banget", "bngit": "banget", "bngt": "banget",
    "bkn": "bukan",
    "org": "orang",
    "dpt": "dapat",
    "lbh": "lebih",

    # Konteks Bencana (PENTING)
    "bnjr": "banjir",
    "evaku": "evakuasi",
    "korbn": "korban",
    "tnda": "tenda",
    "bntuan": "bantuan"
}

# Download resource NLTK jika belum ada
nltk.download('stopwords')
nltk.download('punkt')

# --- 1. Setup Stopwords ---
# Ambil stopwords bawaan bahasa Indonesia
stop_words = set(stopwords.words('indonesian'))

# PENTING: Jangan hapus kata-kata negasi & waktu!
# Tanpa stemming, kita harus lebih hati-hati membuang kata.
keep_words = {
    "tidak", "tapi", "namun", "bukan", "jangan", "belum", "kurang",
    "telah", "sudah", "bisa", "dapat", "akan", "sedang"
}
stop_words = stop_words - keep_words

# Tambahkan kata sampah (interjeksi) yang aman dihapus
extra_stopwords = {
    "nih", "sih", "dong", "nya", "tuh", "deh", "kok", "aja", "saja",
    "yah", "wow", "oh", "hm", "hmm", "yup", "gan", "kak", "bro", "sis",
    "yuk", "loh", "kan", "yang", "dan", "di", "ke", "dari"
}
stop_words.update(extra_stopwords)

# --- 2. Kamus Normalisasi (Pengganti Stemming) ---
# Kita gunakan kamus ini untuk membakukan kata tanpa memotong imbuhan secara paksa
normalisasi_dict = {
    # Negasi & Kata Sambung
    "gk": "tidak", "ga": "tidak", "gak": "tidak", "nggak": "tidak", "tdk": "tidak", "tak": "tidak",
    "klo": "kalau", "kalo": "kalau", "kl": "kalau",
    "dg": "dengan", "dgn": "dengan",
    "yg": "yang",
    "krn": "karena", "karna": "karena",
    "tp": "tapi",
    "sm": "sama",
    "utk": "untuk",
    "dlm": "dalam",
    "dr": "dari",

    # Pronoun (Kata Ganti)
    "sy": "saya", "gw": "saya", "aku": "saya", "gue": "saya", "gua": "saya",
    "lu": "kamu", "lo": "kamu", "u": "kamu",

    # Kata Kerja/Keterangan
    "jd": "jadi", "jdi": "jadi",
    "sdh": "sudah", "udh": "sudah", "dah": "sudah",
    "blm": "belum",
    "tlg": "tolong", "pls": "tolong", "mohon": "tolong", "bntu": "bantu",
    "knp": "kenapa", "napa": "kenapa",
    "gmn": "bagaimana", "gmna": "bagaimana",
    "bgt": "banget", "bngit": "banget", "bngt": "banget",
    "bkn": "bukan",
    "org": "orang",
    "dpt": "dapat",
    "lbh": "lebih",

    # Konteks Bencana (PENTING)
    "bnjr": "banjir",
    "evaku": "evakuasi",
    "korbn": "korban",
    "tnda": "tenda",
    "bntuan": "bantuan"
}

def preprocess_text(text):
    """
    Preprocessing TANPA Sastrawi.
    Tahapan: Lowercase -> Regex Cleaning -> Normalisasi Slang -> Stopword Removal
    """
    if pd.isna(text):
        return ""

    # 1. Case Folding
    text = text.lower()

    # 2. Cleaning Regex
    # Hapus Username
    text = re.sub(r'@[A-Za-z0-9_]+', ' ', text)
    # Hapus URL
    text = re.sub(r"http\S+|www\S+|https\S+", " ", text)
    # Hapus Angka (Ganti dengan spasi agar tidak menempel dengan teks)
    text = re.sub(r"\d+", " ", text)
    # Hapus Tanda Baca
    text = text.translate(str.maketrans("", "", string.punctuation))
    # Hapus Karakter Berulang (misal: "tolooong" -> "tolong")
    text = re.sub(r'(.)\1{2,}', r'\1', text)
    # Hapus Emoji & Karakter Non-ASCII
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)
    # Hapus spasi berlebih
    text = re.sub(r'\s+', ' ', text).strip()

    # 3. Tokenisasi & Normalisasi
    words = text.split()
    clean_words = []

    for word in words:
        # Cek normalisasi (slang -> baku)
        if word in normalisasi_dict:
            word = normalisasi_dict[word]

        # Filter Stopwords
        if word not in stop_words:
            clean_words.append(word)

    # Gabung kembali menjadi kalimat
    return " ".join(clean_words)

def train_and_save_model(file_path):
    """Melatih model klasifikasi dari data training."""

    # Memuat data (menggunakan delimiter ';' sesuai struktur file)
    try:
        df = pd.read_csv(file_path, delimiter=';')
    except Exception as e:
        print(f"Error saat memuat file: {e}")
        return None, None

    # Memastikan kolom yang dibutuhkan ada
    if 'full_text' not in df.columns or 'labels' not in df.columns:
        print("Pastikan file memiliki kolom 'full_text' dan 'labels'.")
        return None, None

    # Pembersihan teks
    df['clean_text'] = df['full_text'].apply(preprocess_text)

    # Mendefinisikan fitur (X) dan target (y)
    X = df['clean_text']
    y = df['labels']

    # Membuat Pipeline untuk Vektorizer dan Classifier
    # Model: Logistic Regression
    text_classifier = Pipeline([
        ('tfidf', TfidfVectorizer()),
        ('classifier', LogisticRegression(random_state=42, max_iter=1000))
    ])

    # Melatih model
    print("Mulai melatih model...")
    text_classifier.fit(X, y)
    print("Pelatihan model selesai.")

    # (Opsional) Evaluasi model di data training untuk melihat performa awal
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    text_classifier.fit(X_train, y_train)
    predictions = text_classifier.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    print("\n--- Akurasi: ", accuracy)
    print("---\n--- Laporan Klasifikasi (di Data Uji) ---")
    print(classification_report(y_test, predictions, target_names=['0 (Opini/Kritik)', '1 (Informasi)', '2 (Aksi/Bantuan)'], zero_division=0))

    return text_classifier

def classify_new_comment(model, new_texts):
    """Menggunakan model yang telah dilatih untuk memprediksi label komentar baru."""

    if model is None:
        print("Model tidak tersedia untuk prediksi.")
        return

    # Mapping Label
    label_map = {
        0: "0 (Opini/Keluhan/Kritik)",
        1: "1 (Informasi)",
        2: "2 (Aksi Pemerintah/Bantuan)"
    }

    # Memproses dan memprediksi
    print("\n--- Hasil Prediksi Komentar Terbaru ---")
    predictions = model.predict(new_texts)

    for text, label in zip(new_texts, predictions):
        print(f"\n[Komentar]: '{text}'")
        print(f"[Prediksi]: {label_map.get(label, 'Label Tidak Dikenal')}")

# --- Bagian Utama Program ---
if __name__ == "__main__":

    # 1. Path file data training Anda
    data_file = 'dataset_komentarbencana.csv'

    # 2. Melatih model
    classifier_model = train_and_save_model(data_file)

    # 3. Data Komentar Terbaru (yang ingin Anda klasifikasikan)
    new_comments = [
        "Banjir masih setinggi lutut",
        "Menurut saya hujan ini tidak akan berhenti sampai pagi",
        "Tim SAR berhasil mengevakuasi 50 warga yang terjebak banjir di perumahan Kemang Pratama tadi malam. #GerakCepat",
        "Bantuan yang diberikan tentara telah tiba",
        "pekerjaan dan gerak dari tim sar sangat lambat, saran saya perbaiki"
    ]

    # 4. Melakukan Klasifikasi
    if classifier_model:
        classify_new_comment(classifier_model, new_comments)