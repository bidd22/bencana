# -*- coding: utf-8 -*-
"""prediksi komentar 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rOxuZEDxujVjDXqIfPKW0jmFM05CRrjn
"""



import pandas as pd
import re
import string
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
import joblib # Untuk menyimpan model
import os

# Download resource NLTK jika belum ada
nltk.download('stopwords')
nltk.download('punkt')

# --- 1. Setup Stopwords ---
# Ambil stopwords bawaan bahasa Indonesia
stop_words = set(stopwords.words('indonesian'))

# PENTING: Jangan hapus kata-kata negasi & waktu!
# Tanpa stemming, kita harus lebih hati-hati membuang kata.
keep_words = {
    "tidak", "tapi", "namun", "bukan", "jangan", "belum", "kurang",
    "telah", "sudah", "bisa", "dapat", "akan", "sedang"
}
stop_words = stop_words - keep_words

# Tambahkan kata sampah (interjeksi) yang aman dihapus
extra_stopwords = {
    "nih", "sih", "dong", "nya", "tuh", "deh", "kok", "aja", "saja",
    "yah", "wow", "oh", "hm", "hmm", "yup", "gan", "kak", "bro", "sis",
    "yuk", "loh", "kan", "yang", "dan", "di", "ke", "dari"
}
stop_words.update(extra_stopwords)

# --- 2. Kamus Normalisasi (Pengganti Stemming) ---
# Kita gunakan kamus ini untuk membakukan kata tanpa memotong imbuhan secara paksa
normalisasi_dict = {
    # Negasi & Kata Sambung
    "gk": "tidak", "ga": "tidak", "gak": "tidak", "nggak": "tidak", "tdk": "tidak", "tak": "tidak",
    "klo": "kalau", "kalo": "kalau", "kl": "kalau",
    "dg": "dengan", "dgn": "dengan",
    "yg": "yang",
    "krn": "karena", "karna": "karena",
    "tp": "tapi",
    "sm": "sama",
    "utk": "untuk",
    "dlm": "dalam",
    "dr": "dari",

    # Pronoun (Kata Ganti)
    "sy": "saya", "gw": "saya", "aku": "saya", "gue": "saya", "gua": "saya",
    "lu": "kamu", "lo": "kamu", "u": "kamu",

    # Kata Kerja/Keterangan
    "jd": "jadi", "jdi": "jadi",
    "sdh": "sudah", "udh": "sudah", "dah": "sudah",
    "blm": "belum",
    "tlg": "tolong", "pls": "tolong", "mohon": "tolong", "bntu": "bantu",
    "knp": "kenapa", "napa": "kenapa",
    "gmn": "bagaimana", "gmna": "bagaimana",
    "bgt": "banget", "bngit": "banget", "bngt": "banget",
    "bkn": "bukan",
    "org": "orang",
    "dpt": "dapat",
    "lbh": "lebih",

    # Konteks Bencana (PENTING)
    "bnjr": "banjir",
    "evaku": "evakuasi",
    "korbn": "korban",
    "tnda": "tenda",
    "bntuan": "bantuan"
}

# Download resource NLTK jika belum ada
nltk.download('stopwords')
nltk.download('punkt')

# --- 1. Setup Stopwords ---
# Ambil stopwords bawaan bahasa Indonesia
stop_words = set(stopwords.words('indonesian'))

# PENTING: Jangan hapus kata-kata negasi & waktu!
# Tanpa stemming, kita harus lebih hati-hati membuang kata.
keep_words = {
    "tidak", "tapi", "namun", "bukan", "jangan", "belum", "kurang",
    "telah", "sudah", "bisa", "dapat", "akan", "sedang"
}
stop_words = stop_words - keep_words

# Tambahkan kata sampah (interjeksi) yang aman dihapus
extra_stopwords = {
    "nih", "sih", "dong", "nya", "tuh", "deh", "kok", "aja", "saja",
    "yah", "wow", "oh", "hm", "hmm", "yup", "gan", "kak", "bro", "sis",
    "yuk", "loh", "kan", "yang", "dan", "di", "ke", "dari"
}
stop_words.update(extra_stopwords)

# --- 2. Kamus Normalisasi (Pengganti Stemming) ---
# Kita gunakan kamus ini untuk membakukan kata tanpa memotong imbuhan secara paksa
normalisasi_dict = {
    # Negasi & Kata Sambung
    "gk": "tidak", "ga": "tidak", "gak": "tidak", "nggak": "tidak", "tdk": "tidak", "tak": "tidak",
    "klo": "kalau", "kalo": "kalau", "kl": "kalau",
    "dg": "dengan", "dgn": "dengan",
    "yg": "yang",
    "krn": "karena", "karna": "karena",
    "tp": "tapi",
    "sm": "sama",
    "utk": "untuk",
    "dlm": "dalam",
    "dr": "dari",

    # Pronoun (Kata Ganti)
    "sy": "saya", "gw": "saya", "aku": "saya", "gue": "saya", "gua": "saya",
    "lu": "kamu", "lo": "kamu", "u": "kamu",

    # Kata Kerja/Keterangan
    "jd": "jadi", "jdi": "jadi",
    "sdh": "sudah", "udh": "sudah", "dah": "sudah",
    "blm": "belum",
    "tlg": "tolong", "pls": "tolong", "mohon": "tolong", "bntu": "bantu",
    "knp": "kenapa", "napa": "kenapa",
    "gmn": "bagaimana", "gmna": "bagaimana",
    "bgt": "banget", "bngit": "banget", "bngt": "banget",
    "bkn": "bukan",
    "org": "orang",
    "dpt": "dapat",
    "lbh": "lebih",

    # Konteks Bencana (PENTING)
    "bnjr": "banjir",
    "evaku": "evakuasi",
    "korbn": "korban",
    "tnda": "tenda",
    "bntuan": "bantuan"
}

def preprocess_text(text):
    """
    Preprocessing TANPA Sastrawi.
    Tahapan: Lowercase -> Regex Cleaning -> Normalisasi Slang -> Stopword Removal
    """
    if pd.isna(text):
        return ""

    # 1. Case Folding
    text = text.lower()

    # 2. Cleaning Regex
    # Hapus Username
    text = re.sub(r'@[A-Za-z0-9_]+', ' ', text)
    # Hapus URL
    text = re.sub(r"http\S+|www\S+|https\S+", " ", text)
    # Hapus Angka (Ganti dengan spasi agar tidak menempel dengan teks)
    text = re.sub(r"\d+", " ", text)
    # Hapus Tanda Baca
    text = text.translate(str.maketrans("", "", string.punctuation))
    # Hapus Karakter Berulang (misal: "tolooong" -> "tolong")
    text = re.sub(r'(.)\1{2,}', r'\1', text)
    # Hapus Emoji & Karakter Non-ASCII
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)
    # Hapus spasi berlebih
    text = re.sub(r'\s+', ' ', text).strip()

    # 3. Tokenisasi & Normalisasi
    words = text.split()
    clean_words = []

    for word in words:
        # Cek normalisasi (slang -> baku)
        if word in normalisasi_dict:
            word = normalisasi_dict[word]

        # Filter Stopwords
        if word not in stop_words:
            clean_words.append(word)

    # Gabung kembali menjadi kalimat
    return " ".join(clean_words)

def train_and_save_model(file_path, output_model_name='model_bencana.pkl'):
    """
    Melatih model klasifikasi dengan konfigurasi yang lebih optimal:
    - Menggunakan N-gram (Unigram + Bigram) untuk menangkap konteks "tidak ada", "kurang bantuan"
    - Mengatasi ketimpangan data dengan class_weight='balanced'
    - Menyimpan model fisik (.pkl)
    """

    # 1. Memuat data
    try:
        # Coba delimiter ';' dulu, jika gagal coba ','
        df = pd.read_csv(file_path, delimiter=';')
        if 'full_text' not in df.columns: # Coba fallback jika delimiter salah
             df = pd.read_csv(file_path, delimiter=',')
    except Exception as e:
        print(f"Error saat memuat file: {e}")
        return None

    # Pastikan kolom ada
    if 'full_text' not in df.columns or 'labels' not in df.columns:
        print("Error: File harus memiliki kolom 'full_text' dan 'labels'.")
        print("Kolom yang ditemukan:", df.columns)
        return None

    print(f"Data dimuat: {len(df)} baris.")

    # 2. Preprocessing
    # Asumsi fungsi 'preprocess_text' sudah didefinisikan sebelumnya
    print("Sedang melakukan preprocessing...")
    df['clean_text'] = df['full_text'].apply(preprocess_text)

    # Hapus data kosong setelah preprocessing (jika ada sisa karakter sampah)
    df = df[df['clean_text'].str.strip().astype(bool)]

    X = df['clean_text']
    y = df['labels']

    # 3. Split Data (80% Train, 20% Test)
    # Stratify=y memastikan proporsi tiap label di training & testing sama
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # 4. Membuat Pipeline
    text_classifier = Pipeline([
        ('tfidf', TfidfVectorizer(
            ngram_range=(1, 2),  # PENTING: Baca 1 kata dan 2 kata berurutan (biar paham "tidak ada")
            max_features=5000,   # Batasi fitur agar tidak terlalu berat
            min_df=2             # Abaikan kata yang muncul kurang dari 2 kali (typo unik)
        )),
        ('classifier', LogisticRegression(
            random_state=42,
            max_iter=1000,
            solver='liblinear',      # Algoritma yang bagus untuk dataset teks ukuran kecil-sedang
            class_weight='balanced'  # PENTING: Agar adil meski jumlah data per kelas tidak sama
        ))
    ])

    # 5. Melatih Model
    print("Melatih model...")
    text_classifier.fit(X_train, y_train)

    # 6. Evaluasi Model
    predictions = text_classifier.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)

    print("\n" + "="*30)
    print(f"Akurasi Model: {accuracy:.2%}")
    print("="*30)
    print("\nLaporan Klasifikasi Detail:")

    # Target names disesuaikan dengan label Anda (0, 1, 2)
    target_names = ['0 (Opini/Kritik)', '1 (Informasi)', '2 (Aksi/Bantuan)']
    print(classification_report(y_test, predictions, target_names=target_names, zero_division=0))

    # 7. Menyimpan Model ke File
    joblib.dump(text_classifier, output_model_name)
    print(f"Model berhasil disimpan ke file: '{output_model_name}'")

    return text_classifier

def classify_new_comment(model, new_texts):
    """
    Menggunakan model untuk memprediksi label komentar baru.
    PENTING: Data baru harus di-preprocess sama seperti data training.
    """
    if model is None:
        print("Model tidak tersedia untuk prediksi.")
        return

    # Mapping Label (Sesuaikan dengan data training Anda)
    label_map = {
        0: "0 (Opini/Keluhan/Kritik)",
        1: "1 (Informasi)",
        2: "2 (Aksi Pemerintah/Bantuan)"
    }

    print("\n" + "="*40)
    print("   HASIL PREDIKSI KOMENTAR TERBARU")
    print("="*40)

    # 1. Preprocessing data baru (Wajib!)
    # Kita harus membersihkan teks input agar formatnya sama dengan data training
    clean_texts = [preprocess_text(text) for text in new_texts]

    # 2. Prediksi
    predictions = model.predict(clean_texts)

    # 3. Tampilkan
    for original, clean, label in zip(new_texts, clean_texts, predictions):
        label_text = label_map.get(label, f"Label {label}")

        print(f"\n[Asli]     : {original}")
        # print(f"[Bersih]   : {clean}") # Uncomment jika ingin debug hasil cleaning
        print(f"[Prediksi] : {label_text}")

# --- Bagian Utama Program ---
if __name__ == "__main__":

    # Konfigurasi File
    data_file = 'dataset_komentarbencana.csv'
    model_file = 'model_bencana.pkl' # Nama file untuk menyimpan model

    classifier_model = None

    # Logika Cerdas: Cek apakah model sudah ada?
    # Jika sudah ada file .pkl, kita load saja (tidak perlu training ulang yang lama)
    if os.path.exists(model_file):
        print(f"File model '{model_file}' ditemukan. Memuat model...")
        try:
            classifier_model = joblib.load(model_file)
            print("Model berhasil dimuat!")
        except Exception as e:
            print(f"Gagal memuat model: {e}. Akan melatih ulang...")
            classifier_model = train_and_save_model(data_file, output_model_name=model_file)
    else:
        print("Model belum ditemukan. Memulai pelatihan baru...")
        classifier_model = train_and_save_model(data_file, output_model_name=model_file)

    # Pastikan model tersedia sebelum prediksi
    if classifier_model:
        # Data Komentar Terbaru (yang ingin Anda klasifikasikan)
        new_comments = [
            "Banjir masih setinggi lutut di depan rumah",
            "Menurut saya hujan ini tidak akan berhenti sampai pagi, parah banget",
            "Tim SAR berhasil mengevakuasi 50 warga yang terjebak banjir di perumahan Kemang Pratama tadi malam. #GerakCepat",
            "Bantuan yang diberikan tentara telah tiba dan dibagikan",
            "pekerjaan dan gerak dari tim sar sangat lambat, tolong perbaiki kerjanya",
            "Mohon info apakah jalur menuju puncak sudah bisa dilewati?"
        ]

        # Lakukan Klasifikasi
        classify_new_comment(classifier_model, new_comments)
    else:
        print("\nProgram berhenti karena model gagal dimuat/dilatih.")
        print(f"Pastikan file '{data_file}' tersedia dan formatnya benar.")